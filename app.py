import streamlit as st
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans
import plotly.express as px
import io
from pytrends.request import TrendReq

# ----------------------------------------------------------------------
# ì‹ ê·œ ê¸°ëŠ¥: ì‹œì¥ íŠ¸ë Œë“œ ë¶„ì„ í•¨ìˆ˜
# ----------------------------------------------------------------------


@st.cache_data(ttl=3600)  # 1ì‹œê°„ ë™ì•ˆ ìºì‹œ ìœ ì§€
def get_daily_trends():
    """Google Trendsì—ì„œ ëŒ€í•œë¯¼êµ­ì˜ ì¼ë³„ ì¸ê¸° ê²€ìƒ‰ì–´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        pytrends = TrendReq(hl='ko-KR', tz=540)
        trending_searches_df = pytrends.trending_searches(pn='south_korea')
        return trending_searches_df[0].tolist()
    except Exception as e:
        # st.errorëŠ” ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œë§Œ í˜¸ì¶œ ê°€ëŠ¥í•˜ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” Noneì„ ë°˜í™˜í•˜ê³  í˜¸ì¶œë¶€ì—ì„œ ì²˜ë¦¬
        return None

# ----------------------------------------------------------------------
# ê¸°ì¡´ ë°ì´í„° ì²˜ë¦¬ ë° ë¶„ì„ í•¨ìˆ˜ë“¤
# ----------------------------------------------------------------------


@st.cache_data
def load_and_process_data(uploaded_file_contents, k, lambda_param, w_amount, w_freq, w_recency):
    """íŒŒì¼ ë‚´ìš©ì„ ì…ë ¥ë°›ì•„ ë°ì´í„° ì²˜ë¦¬ ë° ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    try:
        file_name = st.session_state.get('file_name', '')
        file_extension = file_name.split('.')[-1]

        if file_extension == 'csv':
            df_raw = pd.read_csv(io.BytesIO(
                uploaded_file_contents), low_memory=False)
        else:
            df_raw = pd.read_excel(io.BytesIO(uploaded_file_contents))

        if df_raw.empty:
            return None, "ì˜¤ë¥˜: ì—…ë¡œë“œëœ íŒŒì¼ì— ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
    except Exception as e:
        return None, f"íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

    date_col = next((col for col in df_raw.columns if 'ë‚ ì§œ' in col), 'ë‚ ì§œ')
    book_col = next((col for col in df_raw.columns if 'ë„ì„œëª…' in col), 'ë„ì„œëª…')
    amount_col = next((col for col in df_raw.columns if 'ë°œì£¼ëŸ‰' in col), 'ë°œì£¼ëŸ‰')

    if not all(c in df_raw.columns for c in [date_col, book_col, amount_col]):
        return None, f"ë¶„ì„ì— í•„ìš”í•œ ì»¬ëŸ¼('{date_col}', '{book_col}', '{amount_col}') ì¤‘ ì¼ë¶€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    df_raw[date_col] = pd.to_datetime(df_raw[date_col], errors='coerce')
    df_raw.dropna(subset=[date_col, book_col, amount_col], inplace=True)

    weekday_map = {"Monday": "ì›”", "Tuesday": "í™”", "Wednesday": "ìˆ˜",
                   "Thursday": "ëª©", "Friday": "ê¸ˆ", "Saturday": "í† ", "Sunday": "ì¼"}
    df_raw['ë‚ ì§œ_ë¼ë²¨'] = df_raw[date_col].dt.strftime(
        '%mì›” %dì¼') + " (" + df_raw[date_col].dt.day_name().map(weekday_map).fillna('') + ")"

    agg_df = df_raw.groupby(book_col).agg(
        ì´ë°œì£¼ëŸ‰=(amount_col, 'sum'),
        ë°œì£¼íšŸìˆ˜=(book_col, 'count'),
        ìµœê·¼ë°œì£¼ì¼=(date_col, 'max'),
        ìµœì´ˆë°œì£¼ì¼=(date_col, 'min')
    ).reset_index()

    duration = (agg_df['ìµœê·¼ë°œì£¼ì¼'] - agg_df['ìµœì´ˆë°œì£¼ì¼']).dt.days
    agg_df['í‰ê·  ë°œì£¼ ê°„ê²©'] = np.where(
        agg_df['ë°œì£¼íšŸìˆ˜'] > 1, duration / (agg_df['ë°œì£¼íšŸìˆ˜'] - 1), np.nan)

    def calculate_recency_score(days_diff, max_days, k_val, lambda_val):
        if max_days == 0:
            return 1.0
        scaled_diff = days_diff / max_days
        return 1 / (1 + scaled_diff * k_val) if scaled_diff <= 1 else np.exp(-scaled_diff * lambda_val)

    ê¸°ì¤€ì¼ = agg_df['ìµœê·¼ë°œì£¼ì¼'].max()
    agg_df['ìµœì´ˆë°œì£¼í›„ê²½ê³¼ì¼'] = (ê¸°ì¤€ì¼ - agg_df['ìµœì´ˆë°œì£¼ì¼']).dt.days
    agg_df['ê²½ê³¼ì¼'] = (ê¸°ì¤€ì¼ - agg_df['ìµœê·¼ë°œì£¼ì¼']).dt.days
    max_days = agg_df['ê²½ê³¼ì¼'].max()
    agg_df['ì‹œê°„ê°€ì¤‘ì¹˜'] = agg_df['ê²½ê³¼ì¼'].apply(
        lambda x: calculate_recency_score(x, max_days, k, lambda_param))

    features = ['ì´ë°œì£¼ëŸ‰', 'ë°œì£¼íšŸìˆ˜', 'ì‹œê°„ê°€ì¤‘ì¹˜']
    scaler = MinMaxScaler()

    norm_features = [f + '_ì •ê·œí™”' for f in features]
    agg_df[norm_features] = scaler.fit_transform(agg_df[features])

    kmeans = KMeans(n_clusters=5, init='k-means++',
                    n_init=10, max_iter=300, random_state=42)
    agg_df['Cluster'] = kmeans.fit_predict(agg_df[norm_features])

    centroids_df_normalized = pd.DataFrame(
        kmeans.cluster_centers_, columns=norm_features)
    rank_score_series = (w_amount * centroids_df_normalized['ì´ë°œì£¼ëŸ‰_ì •ê·œí™”'] +
                         w_freq * centroids_df_normalized['ë°œì£¼íšŸìˆ˜_ì •ê·œí™”'] +
                         w_recency * centroids_df_normalized['ì‹œê°„ê°€ì¤‘ì¹˜_ì •ê·œí™”'])
    centroids_df_normalized['rank_score'] = rank_score_series

    sorted_clusters = centroids_df_normalized['rank_score'].sort_values(
        ascending=False).index

    grade_map = {cluster_id: f"{i+1}ë“±ê¸‰" for i,
                 cluster_id in enumerate(sorted_clusters)}
    agg_df['ë“±ê¸‰'] = agg_df['Cluster'].map(grade_map)

    total_weight = w_amount + w_freq + w_recency
    agg_df['ì¢…í•© ì ìˆ˜'] = (
        (w_amount * agg_df['ì´ë°œì£¼ëŸ‰_ì •ê·œí™”'] +
         w_freq * agg_df['ë°œì£¼íšŸìˆ˜_ì •ê·œí™”'] +
         w_recency * agg_df['ì‹œê°„ê°€ì¤‘ì¹˜_ì •ê·œí™”']) / total_weight
    ) * 100

    agg_df.drop(columns=norm_features, inplace=True)

    return agg_df, df_raw


def to_excel(df):
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        df.to_excel(writer, index=False, sheet_name='analysis_result')
    return output.getvalue()


# ----------------------------------------------------------------------
# Streamlit ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ UI
# ----------------------------------------------------------------------
st.set_page_config(layout="wide", page_title="ë„ì„œ ë°œì£¼ ë°ì´í„° ë¶„ì„ ëŒ€ì‹œë³´ë“œ")
st.title("ğŸ“š ë„ì„œ ë°œì£¼ ë°ì´í„° ë¶„ì„ ëŒ€ì‹œë³´ë“œ")

if 'analysis_done' not in st.session_state:
    st.session_state.analysis_done = False

with st.sidebar:
    st.header("âš™ï¸ 1. ë¶„ì„ íŒŒì¼ ì—…ë¡œë“œ")
    uploaded_file = st.file_uploader(
        "CSV ë˜ëŠ” XLSX ë°œì£¼ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", type=["csv", "xlsx"])

    if uploaded_file:
        if 'file_name' not in st.session_state or st.session_state.file_name != uploaded_file.name:
            st.session_state.analysis_done = False
            st.session_state.file_name = uploaded_file.name
            st.session_state.file_contents = uploaded_file.getvalue()

        try:
            date_col_name = 'ë‚ ì§œ'
            file_contents_for_preview = io.BytesIO(
                st.session_state.file_contents)
            if st.session_state.file_name.endswith('.csv'):
                df_preview = pd.read_csv(
                    file_contents_for_preview, low_memory=False, usecols=[date_col_name])
            else:
                df_preview = pd.read_excel(
                    file_contents_for_preview, usecols=[date_col_name])
            df_preview[date_col_name] = pd.to_datetime(
                df_preview[date_col_name], errors='coerce').dropna()
            time_span_days = (df_preview[date_col_name].max(
            ) - df_preview[date_col_name].min()).days if not df_preview.empty else 0
        except Exception:
            time_span_days = 365

        if time_span_days <= 7:
            k_default, lambda_default, period_text = 7.0, 0.5, "1ì£¼ì¼ ì´ë‚´"
        elif time_span_days <= 31:
            k_default, lambda_default, period_text = 5.0, 0.7, "1ê°œì›” ì´ë‚´"
        elif time_span_days <= 182:
            k_default, lambda_default, period_text = 2.0, 1.0, "6ê°œì›” ì´ë‚´"
        elif time_span_days <= 365:
            k_default, lambda_default, period_text = 1.0, 1.5, "1ë…„ ì´ë‚´"
        else:
            k_default, lambda_default, period_text = 0.5, 2.0, "1ë…„ ì´ìƒ"

        st.header("âš™ï¸ 2. ë¶„ì„ ë¯¼ê°ë„ ì„¤ì •")
        st.info(f"ë°ì´í„° ê¸°ê°„: **{period_text}**")
        k_param = st.slider("ìµœì‹ ì„± ë¯¼ê°ë„ (k)", 0.1, 10.0, k_default, 0.1,
                            help="ê°’ì´ í´ìˆ˜ë¡ 'ìµœê·¼'ì— ë°œì£¼ëœ ë„ì„œì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤. ë‹¨ê¸° ë°ì´í„°ë¥¼ ë¶„ì„í•  ë•Œ ë†’ê²Œ ì„¤ì •í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.")
        lambda_param = st.slider("ì¥ê¸° ë¹„í™œì„± íŒ¨ë„í‹° (Î»)", 0.1, 10.0, lambda_default, 0.1,
                                 help="ê°’ì´ í´ìˆ˜ë¡ ë°œì£¼ëœ ì§€ 'ì•„ì£¼ ì˜¤ë˜ëœ' ë„ì„œì— ëŒ€í•œ íŒ¨ë„í‹°ë¥¼ ê°•í•˜ê²Œ ë¶€ì—¬í•©ë‹ˆë‹¤. ì¥ê¸° ë°ì´í„°ë¥¼ ë¶„ì„í•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.")

        st.header("âš™ï¸ 3. ë“±ê¸‰/ì ìˆ˜ ì¤‘ìš”ë„ ì„¤ì •")
        w_amount = st.slider("ì´ë°œì£¼ëŸ‰ ì¤‘ìš”ë„", 1, 5, 4,
                             help="ì´ ê°’ì´ ë†’ì„ìˆ˜ë¡ 'ë§ì´' íŒ”ë¦° ì±…ì´ ë†’ì€ ë“±ê¸‰/ì ìˆ˜ë¥¼ ë°›ìŠµë‹ˆë‹¤.")
        w_freq = st.slider("ë°œì£¼íšŸìˆ˜ ì¤‘ìš”ë„", 1, 5, 4,
                           help="ì´ ê°’ì´ ë†’ì„ìˆ˜ë¡ 'ìì£¼' íŒ”ë¦° ì±…ì´ ë†’ì€ ë“±ê¸‰/ì ìˆ˜ë¥¼ ë°›ìŠµë‹ˆë‹¤.")
        w_recency = st.slider("ì‹œê°„ê°€ì¤‘ì¹˜ ì¤‘ìš”ë„", 1, 5, 2,
                              help="ì´ ê°’ì´ ë†’ì„ìˆ˜ë¡ 'ìµœê·¼ì—' íŒ”ë¦° ì±…ì´ ë†’ì€ ë“±ê¸‰/ì ìˆ˜ë¥¼ ë°›ìŠµë‹ˆë‹¤.")

        if st.button("ğŸš€ ë¶„ì„ ì‹¤í–‰", type="primary", use_container_width=True):
            with st.spinner('ë°ì´í„°ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...'):
                agg_df, df_raw_or_error = load_and_process_data(
                    st.session_state.file_contents, k_param, lambda_param, w_amount, w_freq, w_recency)
                if agg_df is not None:
                    st.session_state.analysis_done = True
                    st.session_state.agg_df = agg_df
                    st.session_state.df_raw = df_raw_or_error
                else:
                    st.session_state.analysis_done = False
                    st.error(df_raw_or_error)
            st.rerun()

if not uploaded_file:
    st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ ë¶„ì„í•  íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
elif st.session_state.analysis_done:
    agg_df = st.session_state.agg_df
    df_raw = st.session_state.df_raw

    st.success(f"âœ… **{st.session_state.file_name}** íŒŒì¼ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

    tab1, tab2, tab3, tab4, tab5 = st.tabs(
        ["[ ğŸ“ˆ ë“±ê¸‰ ìš”ì•½ ]", "[ ğŸŒŸ ìœ ë§ ë„ì„œ ë°œêµ´ ]", "[ ğŸ“Š ë°ì´í„° ì¸ì‚¬ì´íŠ¸ ]", "[ ğŸ” ì‹œì¥ íŠ¸ë Œë“œ ë¶„ì„ ]", "[ ğŸ“‹ ì „ì²´ ë°ì´í„° ]"])

    with tab1:
        st.header("ë“±ê¸‰ë³„ ìš”ì•½")
        col1, col2 = st.columns(2)
        with col1:
            st.subheader("â‘  ë“±ê¸‰ë³„ ë„ì„œ ë¶„í¬")
            grade_order = [f"{i}ë“±ê¸‰" for i in range(1, 6)]
            grade_counts = agg_df['ë“±ê¸‰'].value_counts().reindex(
                grade_order).reset_index()
            fig_bar = px.bar(grade_counts, x='ë“±ê¸‰', y='count', color='ë“±ê¸‰', text_auto=True, category_orders={"ë“±ê¸‰": grade_order},
                             color_discrete_map={'1ë“±ê¸‰': '#0081CF', '2ë“±ê¸‰': '#00A1E0', '3ë“±ê¸‰': '#7ECEF4', '4ë“±ê¸‰': '#B1DFF7', '5ë“±ê¸‰': '#CCCCCC'}, title="í¬íŠ¸í´ë¦¬ì˜¤ ë“±ê¸‰ë³„ ë„ì„œ ìˆ˜")
            st.plotly_chart(fig_bar, use_container_width=True)

        with col2:
            st.subheader("â‘¡ ë“±ê¸‰ë³„ ì˜ë¯¸ì™€ ì „ëµ")
            st.markdown("""
            | ë“±ê¸‰ | ì˜ë¯¸ ë° ìƒíƒœ | ì¶”ì²œ ì „ëµ |
            | :---: | :--- | :--- |
            | **1ë“±ê¸‰** | **ìµœìƒìœ„ í•µì‹¬ ê·¸ë£¹** | **ì¬ê³  ìµœìš°ì„  í™•ë³´**, ì ê·¹ì ì¸ ë§ˆì¼€íŒ… |
            | **2ë“±ê¸‰** | ê¾¸ì¤€í•œ **ìš°ìˆ˜ ê·¸ë£¹** | **ì•ˆì •ì ì¸ ì¬ê³  ìˆ˜ì¤€ ìœ ì§€**, í¬ë¡œìŠ¤ì…€ë§ |
            | **3ë“±ê¸‰** | **ì„±ì¥/í•˜ë½ ê°€ëŠ¥ì„±** ë³´ìœ  ê·¸ë£¹ | ìˆ˜ìš” ì˜ˆì¸¡, íŒë§¤ ì´‰ì§„ ì „ëµ ê³ ë¯¼ |
            | **4ë“±ê¸‰** | ë°œì£¼ê°€ ëœ¸í•œ **ì£¼ì˜ ê·¸ë£¹** | **ì¬ê³  ìµœì†Œí™”**, ì›ì¸ ë¶„ì„ |
            | **5ë“±ê¸‰** | **ë¹„í™œì„±/ê´€ë¦¬ ê·¸ë£¹** | **ì¬ê³  ì²˜ë¶„ ë° ë‹¨ì¢…** ê²€í†  |
            """)

    with tab2:
        st.header("ğŸŒŸ ì‹ ê·œ ìœ ë§ ë„ì„œ ë°œêµ´")
        book_col_name = next(
            (col for col in df_raw.columns if 'ë„ì„œëª…' in col), 'ë„ì„œëª…')

        col1, col2 = st.columns(2)
        with col1:
            days_since_first_limit = st.slider("ì¶œì‹œ ê¸°ê°„ í•„í„° (ì¼)", 0, int(
                agg_df['ìµœì´ˆë°œì£¼í›„ê²½ê³¼ì¼'].max()), min(180, int(agg_df['ìµœì´ˆë°œì£¼í›„ê²½ê³¼ì¼'].max())))
        with col2:
            min_freq_limit = st.slider(
                "ìµœì†Œ ë°œì£¼ íšŸìˆ˜ í•„í„°", 1, int(agg_df['ë°œì£¼íšŸìˆ˜'].max()), 3)

        col3, col4 = st.columns(2)
        with col3:
            max_interval = int(agg_df['í‰ê·  ë°œì£¼ ê°„ê²©'].dropna().max(
            )) if not agg_df['í‰ê·  ë°œì£¼ ê°„ê²©'].isna().all() else 90
            interval_limit = st.slider(
                "ìµœëŒ€ í‰ê·  ë°œì£¼ ê°„ê²© í•„í„°", 1, max_interval, min(30, max_interval))
        with col4:
            min_amount_limit = st.slider(
                "ìµœì†Œ ì´ ë°œì£¼ëŸ‰ í•„í„°", 0, int(agg_df['ì´ë°œì£¼ëŸ‰'].max()), 10)

        promising_books_df = agg_df[
            (agg_df['ìµœì´ˆë°œì£¼í›„ê²½ê³¼ì¼'] <= days_since_first_limit) &
            (agg_df['ë°œì£¼íšŸìˆ˜'] >= min_freq_limit) &
            (agg_df['í‰ê·  ë°œì£¼ ê°„ê²©'].fillna(interval_limit + 1) <= interval_limit) &
            (agg_df['ì´ë°œì£¼ëŸ‰'] >= min_amount_limit)
        ]

        st.subheader(f"í•„í„°ë§ ê²°ê³¼: ì´ {len(promising_books_df)}ê¶Œì˜ ìœ ë§ ë„ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

        col_sort1, col_sort2 = st.columns(2)
        with col_sort1:
            sort_by_options = {"ì¢…í•© ì ìˆ˜": "ì¢…í•© ì ìˆ˜", "í‰ê·  ë°œì£¼ ê°„ê²©": "í‰ê·  ë°œì£¼ ê°„ê²©",
                               "ì´ë°œì£¼ëŸ‰": "ì´ë°œì£¼ëŸ‰", "ë°œì£¼ íšŸìˆ˜": "ë°œì£¼íšŸìˆ˜", "ì¶œì‹œì¼": "ìµœì´ˆë°œì£¼í›„ê²½ê³¼ì¼"}
            sort_by = st.selectbox(
                "ì •ë ¬ ê¸°ì¤€ ì„ íƒ", options=list(sort_by_options.keys()))
        with col_sort2:
            sort_order = st.selectbox("ì •ë ¬ ìˆœì„œ ì„ íƒ", options=["ë‚´ë¦¼ì°¨ìˆœ", "ì˜¤ë¦„ì°¨ìˆœ"])

        promising_books_df = promising_books_df.sort_values(
            by=sort_by_options[sort_by], ascending=(sort_order == "ì˜¤ë¦„ì°¨ìˆœ"))

        for _, row in promising_books_df.iterrows():
            book_title = row[book_col_name]
            with st.expander(f"'{book_title}' (ì¢…í•©ì ìˆ˜: {row['ì¢…í•© ì ìˆ˜']:.2f}ì  / í‰ê·  {row['í‰ê·  ë°œì£¼ ê°„ê²©']:.1f}ì¼ ê°„ê²©)"):
                history_df = df_raw[df_raw[book_col_name] == book_title].copy()
                daily_history = history_df.groupby(next(col for col in history_df.columns if 'ë‚ ì§œ' in col)).agg(
                    ì¼ì¼_ë°œì£¼ëŸ‰=(
                        next(col for col in history_df.columns if 'ë°œì£¼ëŸ‰' in col), 'sum'),
                    ë‚ ì§œ_ë¼ë²¨=('ë‚ ì§œ_ë¼ë²¨', 'first')
                ).reset_index()

                fig = px.line(daily_history, x='ë‚ ì§œ_ë¼ë²¨', y='ì¼ì¼_ë°œì£¼ëŸ‰', title=f"'{book_title}' ë°œì£¼ëŸ‰ ì¶”ì´", markers=True, labels={
                              'ë‚ ì§œ_ë¼ë²¨': 'ë‚ ì§œ', 'ì¼ì¼_ë°œì£¼ëŸ‰': 'ë°œì£¼ëŸ‰'})
                st.plotly_chart(fig, use_container_width=True)

    with tab3:
        st.header("ë°ì´í„° ì¸ì‚¬ì´íŠ¸ ì‹œê°í™”")
        date_col = next((col for col in df_raw.columns if 'ë‚ ì§œ' in col), 'ë‚ ì§œ')
        amount_col = next(
            (col for col in df_raw.columns if 'ë°œì£¼ëŸ‰' in col), 'ë°œì£¼ëŸ‰')

        viz_tab1, viz_tab2, viz_tab3 = st.tabs(
            ["ì›”ë³„ ë°œì£¼ í˜„í™©", "ì£¼ë³„ ë°œì£¼ í˜„í™©", "ì¼ë³„ ë°œì£¼ í˜„í™©"])

        with viz_tab1:
            st.subheader("ì›”ë³„ ì´ ë°œì£¼ëŸ‰")
            monthly_orders = df_raw.groupby(pd.Grouper(key=date_col, freq='ME')).agg(
                í•©ê³„=(amount_col, 'sum')).reset_index()
            monthly_orders['ë‚ ì§œ_ë¼ë²¨'] = monthly_orders[date_col].dt.strftime(
                '%Yë…„ %mì›”')
            fig_month = px.line(monthly_orders, x='ë‚ ì§œ_ë¼ë²¨', y='í•©ê³„', title="ì›”ë³„ ì´ ë°œì£¼ëŸ‰",
                                markers=True, labels={'ë‚ ì§œ_ë¼ë²¨': 'ë‚ ì§œ', 'í•©ê³„': 'ë°œì£¼ëŸ‰'})
            st.plotly_chart(fig_month, use_container_width=True)

        with viz_tab2:
            st.subheader("ì£¼ë³„ ì´ ë°œì£¼ëŸ‰")
            weekly_orders = df_raw.groupby(pd.Grouper(
                key=date_col, freq='W-MON')).agg(í•©ê³„=(amount_col, 'sum')).reset_index()
            weekly_orders['ë‚ ì§œ_ë¼ë²¨'] = weekly_orders[date_col].dt.strftime(
                '%mì›” %dì¼')
            fig_week = px.line(weekly_orders, x='ë‚ ì§œ_ë¼ë²¨', y='í•©ê³„', title="ì£¼ë³„ ì´ ë°œì£¼ëŸ‰",
                               markers=True, labels={'ë‚ ì§œ_ë¼ë²¨': 'ë‚ ì§œ', 'í•©ê³„': 'ë°œì£¼ëŸ‰'})
            st.plotly_chart(fig_week, use_container_width=True)

        with viz_tab3:
            st.subheader("ì¼ë³„ ì´ ë°œì£¼ëŸ‰")
            daily_orders = df_raw.groupby(date_col).agg(
                í•©ê³„=(amount_col, 'sum')).reset_index()
            daily_orders['ë‚ ì§œ_ë¼ë²¨'] = daily_orders[date_col].dt.strftime(
                '%mì›” %dì¼ (%a)')
            fig_day = px.line(daily_orders, x='ë‚ ì§œ_ë¼ë²¨', y='í•©ê³„', title="ì¼ë³„ ì´ ë°œì£¼ëŸ‰", labels={
                              'ë‚ ì§œ_ë¼ë²¨': 'ë‚ ì§œ', 'í•©ê³„': 'ë°œì£¼ëŸ‰'})
            st.plotly_chart(fig_day, use_container_width=True)

    with tab4:
        st.header("ğŸ” ì‹œì¥ íŠ¸ë Œë“œ ë¶„ì„ (Google Trends)")
        st.info("í˜„ì¬ ëŒ€í•œë¯¼êµ­ì—ì„œ ì¸ê¸° ìˆëŠ” ê²€ìƒ‰ì–´ì™€ ê´€ë ¨ëœ ë„ì„œë¥¼ ì°¾ì•„ ìˆ˜ìš”ë¥¼ ì˜ˆì¸¡í•´ ë³´ì„¸ìš”.")

        trending_keywords = get_daily_trends()

        if trending_keywords is None:
            st.error("Google íŠ¸ë Œë“œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        elif not trending_keywords:
            st.warning("í˜„ì¬ ì¸ê¸° ê²€ìƒ‰ì–´ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            col1, col2 = st.columns([1, 2])
            with col1:
                st.subheader("ì˜¤ëŠ˜ì˜ ì¸ê¸° ê²€ìƒ‰ì–´")
                st.dataframe(trending_keywords, hide_index=True,
                             use_container_width=True, column_config={"0": "í‚¤ì›Œë“œ"})

            with col2:
                st.subheader("ê´€ë ¨ ë„ì„œ ëª©ë¡")
                selected_keyword = st.selectbox(
                    "ë¶„ì„í•  ì¸ê¸° ê²€ìƒ‰ì–´ë¥¼ ì„ íƒí•˜ì„¸ìš”:", trending_keywords)

                if selected_keyword:
                    book_col_name = next(
                        (col for col in agg_df.columns if 'ë„ì„œëª…' in col), 'ë„ì„œëª…')
                    matched_books = agg_df[agg_df[book_col_name].str.contains(
                        selected_keyword, case=False, na=False)]

                    if matched_books.empty:
                        st.write(
                            f"**'{selected_keyword}'** í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë„ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    else:
                        display_cols = ['ë„ì„œëª…', 'ë“±ê¸‰', 'ì¢…í•© ì ìˆ˜', 'ì´ë°œì£¼ëŸ‰', 'ìµœê·¼ë°œì£¼ì¼']
                        st.dataframe(
                            matched_books[display_cols].sort_values(
                                by="ì¢…í•© ì ìˆ˜", ascending=False).style.format({'ì¢…í•© ì ìˆ˜': "{:.2f}"}),
                            use_container_width=True
                        )

    with tab5:
        st.header("ì „ì²´ ë¶„ì„ ë°ì´í„°")
        display_columns = ['ë„ì„œëª…', 'ë“±ê¸‰', 'ì¢…í•© ì ìˆ˜', 'ì´ë°œì£¼ëŸ‰',
                           'ë°œì£¼íšŸìˆ˜', 'ì‹œê°„ê°€ì¤‘ì¹˜', 'í‰ê·  ë°œì£¼ ê°„ê²©', 'ìµœì´ˆë°œì£¼ì¼', 'ìµœê·¼ë°œì£¼ì¼', 'ê²½ê³¼ì¼']
        final_df = agg_df[display_columns].sort_values(
            by='ì¢…í•© ì ìˆ˜', ascending=False)
        st.dataframe(final_df.style.format(
            {'ì¢…í•© ì ìˆ˜': "{:.2f}", 'ì‹œê°„ê°€ì¤‘ì¹˜': "{:.3f}", 'í‰ê·  ë°œì£¼ ê°„ê²©': "{:.1f}"}), use_container_width=True)

        st.subheader("ê²°ê³¼ ë‹¤ìš´ë¡œë“œ")
        col1, col2 = st.columns(2)
        with col1:
            csv_data = final_df.to_csv(index=False).encode('utf-8-sig')
            st.download_button("ğŸ’¾ CSV íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œ", data=csv_data,
                               file_name='book_cluster_analysis.csv', mime='text/csv', use_container_width=True)
        with col2:
            excel_data = to_excel(final_df)
            st.download_button("ğŸ’¾ XLSX íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œ", data=excel_data, file_name='book_cluster_analysis.xlsx',
                               mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet', use_container_width=True)
else:
    st.info("ì‚¬ì´ë“œë°”ì—ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì„¤ì •ì„ ë§ˆì¹œ í›„ 'ë¶„ì„ ì‹¤í–‰' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
